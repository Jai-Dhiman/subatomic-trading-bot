{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual-Transformer Energy Trading System - Training Notebook\n",
    "\n",
    "This notebook trains two specialized transformer models:\n",
    "1. **Consumption Transformer**: Predicts household energy consumption\n",
    "2. **Trading Transformer**: Makes optimal buy/sell/hold trading decisions\n",
    "\n",
    "## Architecture\n",
    "- Consumption Transformer: 384d, 6 heads, 5 layers \u2192 predicts next 24h consumption\n",
    "- Trading Transformer: 512d, 8 heads, 6 layers \u2192 predicts prices & trading decisions\n",
    "\n",
    "## Success Criteria\n",
    "- Consumption MAPE < 15%\n",
    "- Price MAE < $0.05/kWh\n",
    "- Cost savings: 20-40% vs baseline\n",
    "- Battery constraints respected (20-90% SoC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Environment\n",
    "\n",
    "Install dependencies, mount Google Drive, verify GPU, set random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SECTION 1: SETUP & ENVIRONMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"\\n1. Mounting Google Drive...\")\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"   \u2713 Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository from GitHub\n",
    "print(\"\\n2. Cloning repository from GitHub...\")\n",
    "import os\n",
    "\n",
    "# Check if repo already exists\n",
    "if os.path.exists('/content/subatomic-trading-bot'):\n",
    "    print(\"   Repository already exists, pulling latest changes...\")\n",
    "    !cd /content/subatomic-trading-bot && git pull\n",
    "else:\n",
    "    print(\"   Cloning repository...\")\n",
    "    !git clone https://github.com/Jai-Dhiman/subatomic-trading-bot.git /content/subatomic-trading-bot\n",
    "\n",
    "# Navigate to project directory\n",
    "os.chdir('/content/subatomic-trading-bot')\n",
    "print(f\"   \u2713 Working directory: {os.getcwd()}\")\n",
    "\n",
    "# List directory to verify\n",
    "print(\"   \u2713 Repository contents:\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy .env file from Google Drive\n",
    "print(\"\\n3. Setting up environment variables...\")\n",
    "import shutil\n",
    "\n",
    "# Copy .env from Google Drive to project directory\n",
    "env_source = '/content/drive/MyDrive/energymvp/.env'\n",
    "env_dest = '/content/subatomic-trading-bot/.env'\n",
    "\n",
    "if os.path.exists(env_source):\n",
    "    shutil.copy(env_source, env_dest)\n",
    "    print(f\"   \u2713 Copied .env from Google Drive\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"\u274c .env file not found at {env_source}. \"\n",
    "        \"Please create it in your Google Drive with SUPABASE_URL and SUPABASE_KEY\"\n",
    "    )\n",
    "\n",
    "# Verify .env exists\n",
    "print(\"   \u2713 Environment file ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies using uv (user preference)\n",
    "print(\"\\n4. Installing dependencies...\")\n",
    "!pip install -q uv\n",
    "\n",
    "# Install PyTorch with CUDA support\n",
    "!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install other dependencies\n",
    "!uv pip install pandas numpy scikit-learn matplotlib seaborn supabase python-dotenv tqdm\n",
    "print(\"   \u2713 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "print(\"\\n5. Setting random seeds...\")\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Deterministic operations (slight performance cost)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"   \u2713 Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(\"\\n6. Checking GPU availability...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB\")\n",
    "    print(f\"   Memory cached: {torch.cuda.memory_reserved(0) / 1024**2:.1f} MB\")\n",
    "else:\n",
    "    print(\"   \u26a0\ufe0f WARNING: GPU not available, using CPU (training will be slow!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "print(\"\\n7. Importing project modules...\")\n",
    "import sys\n",
    "sys.path.append('/content/subatomic-trading-bot')\n",
    "\n",
    "# Data loading\n",
    "from src.data_integration.data_adapter import (\n",
    "    load_consumption_data,\n",
    "    load_pricing_data,\n",
    "    generate_battery_data,\n",
    "    merge_all_data\n",
    ")\n",
    "\n",
    "# Feature engineering\n",
    "from src.models.feature_engineering_consumption import ConsumptionFeatureEngineer\n",
    "from src.models.feature_engineering_trading import TradingFeatureEngineer\n",
    "\n",
    "# Models\n",
    "from src.models.consumption_transformer import (\n",
    "    ConsumptionTransformer,\n",
    "    ConsumptionLoss,\n",
    "    calculate_mape\n",
    ")\n",
    "from src.models.trading_transformer import (\n",
    "    TradingTransformer,\n",
    "    TradingLoss\n",
    ")\n",
    "\n",
    "# Training utilities\n",
    "from src.training.training_utils import (\n",
    "    create_data_loaders,\n",
    "    train_epoch,\n",
    "    validate,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint\n",
    ")\n",
    "\n",
    "# Trading optimizer\n",
    "from src.models.trading_optimizer import calculate_optimal_trading_decisions\n",
    "\n",
    "print(\"   \u2713 All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "print(\"\\n8. Setting up logging...\")\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create checkpoints directory in Google Drive\n",
    "os.makedirs('/content/drive/MyDrive/energymvp/checkpoints', exist_ok=True)\n",
    "print(\"   \u2713 Checkpoints directory: /content/drive/MyDrive/energymvp/checkpoints\")\n",
    "print(\"   \u2713 Logging configured\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 SETUP COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Loading from Supabase\n",
    "\n",
    "Load real consumption data, pricing data, and battery trading labels from Supabase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SECTION 2: DATA LOADING FROM SUPABASE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load environment variables\n",
    "print(\"\\n1. Loading environment variables...\")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Verify Supabase credentials\n",
    "supabase_url = os.getenv('SUPABASE_URL')\n",
    "supabase_key = os.getenv('SUPABASE_KEY')\n",
    "\n",
    "if not supabase_url or not supabase_key:\n",
    "    raise ValueError(\n",
    "        \"\u274c SUPABASE_URL and SUPABASE_KEY must be set in .env file. \"\n",
    "        \"Please copy from your local .env to Colab.\"\n",
    "    )\n",
    "\n",
    "print(f\"   \u2713 Supabase URL: {supabase_url[:30]}...\")\n",
    "print(\"   \u2713 Supabase key configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load consumption data from Supabase\n",
    "print(\"\\n2. Loading consumption data from Supabase...\")\n",
    "print(\"   (This will load from august11homeconsumption table)\")\n",
    "\n",
    "consumption_df = load_consumption_data(source='supabase')\n",
    "\n",
    "print(f\"   \u2713 Loaded {len(consumption_df):,} consumption records\")\n",
    "print(f\"   \u2713 Date range: {consumption_df['timestamp'].min()} to {consumption_df['timestamp'].max()}\")\n",
    "print(f\"   \u2713 Houses: {sorted(consumption_df['house_id'].unique())}\")\n",
    "print(f\"   \u2713 Columns: {consumption_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pricing data from Supabase\n",
    "print(\"\\n3. Loading pricing data from Supabase...\")\n",
    "print(\"   (This will load from cabuyingpricehistoryseptember2025 table with LMP filter)\")\n",
    "\n",
    "pricing_df = load_pricing_data()\n",
    "\n",
    "print(f\"   \u2713 Loaded {len(pricing_df):,} pricing records\")\n",
    "print(f\"   \u2713 Price range: ${pricing_df['price_per_kwh'].min():.4f} to ${pricing_df['price_per_kwh'].max():.4f} per kWh\")\n",
    "print(f\"   \u2713 Mean price: ${pricing_df['price_per_kwh'].mean():.4f} per kWh\")\n",
    "print(f\"   \u2713 Median price: ${pricing_df['price_per_kwh'].median():.4f} per kWh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate battery data\n",
    "print(\"\\n4. Generating synthetic battery data...\")\n",
    "print(\"   (This creates battery state data aligned with consumption timestamps)\")\n",
    "\n",
    "battery_df = generate_battery_data(\n",
    "    timestamps=consumption_df['timestamp'],\n",
    "    consumption_data=consumption_df\n",
    ")\n",
    "\n",
    "print(f\"   \u2713 Generated {len(battery_df):,} battery state records\")\n",
    "print(f\"   \u2713 Battery SoC range: {battery_df['battery_soc_percent'].min():.1f}% to {battery_df['battery_soc_percent'].max():.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all data sources\n",
    "print(\"\\n5. Merging all data sources...\")\n",
    "print(\"   (Aligning consumption, pricing, and battery data by timestamp)\")\n",
    "\n",
    "df_complete = merge_all_data(consumption_df, pricing_df, battery_df)\n",
    "\n",
    "print(f\"   \u2713 Complete dataset: {len(df_complete):,} records\")\n",
    "print(f\"   \u2713 Total columns: {len(df_complete.columns)}\")\n",
    "print(f\"   \u2713 Missing values: {df_complete.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "print(\"\\n6. Sample data:\")\n",
    "print(df_complete.head())\n",
    "\n",
    "print(\"\\n7. Data statistics:\")\n",
    "print(df_complete.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data quality\n",
    "print(\"\\n8. Validating data quality...\")\n",
    "\n",
    "# Check for NaN/Inf values\n",
    "assert not df_complete.isnull().any().any(), \"\u274c Found NaN values in dataset!\"\n",
    "assert not np.isinf(df_complete.select_dtypes(include=[np.number])).any().any(), \"\u274c Found Inf values in dataset!\"\n",
    "\n",
    "# Check for required columns\n",
    "required_cols = [\n",
    "    'timestamp', 'house_id', 'total_consumption_kwh',\n",
    "    'price_per_kwh', 'battery_soc_percent'\n",
    "]\n",
    "for col in required_cols:\n",
    "    assert col in df_complete.columns, f\"\u274c Missing required column: {col}\"\n",
    "\n",
    "print(\"   \u2713 No NaN/Inf values detected\")\n",
    "print(\"   \u2713 All required columns present\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 DATA LOADING COMPLETE\")\n",
    "print(f\"Ready for training with {len(df_complete):,} samples\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Preflight Validation (CPU)\n",
    "\n",
    "Test models and data pipeline on CPU before expensive GPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SECTION 3: PREFLIGHT VALIDATION - CPU ONLY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nThis section validates everything works BEFORE GPU training.\")\n",
    "print(\"We'll run quick tests on CPU to catch errors early.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Consumption Feature Engineering\n",
    "print(\"\\n1. Testing Consumption Feature Engineering...\")\n",
    "\n",
    "engineer_consumption = ConsumptionFeatureEngineer()\n",
    "features_consumption = engineer_consumption.prepare_features(df_complete, fit=True)\n",
    "\n",
    "print(f\"   \u2713 Features extracted: {features_consumption.shape}\")\n",
    "print(f\"   \u2713 Expected: (n_samples, 17)\")\n",
    "assert features_consumption.shape[1] == 17, f\"\u274c Expected 17 features, got {features_consumption.shape[1]}\"\n",
    "\n",
    "# Check for NaN/Inf\n",
    "assert not np.isnan(features_consumption).any(), \"\u274c NaN values in consumption features!\"\n",
    "assert not np.isinf(features_consumption).any(), \"\u274c Inf values in consumption features!\"\n",
    "print(\"   \u2713 No NaN/Inf values in features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Create sequences for Consumption Transformer\n",
    "print(\"\\n2. Creating sequences for Consumption Transformer...\")\n",
    "\n",
    "# Create sequences using feature engineer\n",
    "X_cons, y_cons = engineer_consumption.create_sequences(\n",
    "    features_consumption,\n",
    "    df_complete['total_consumption_kwh'].values,\n",
    "    sequence_length=48,\n",
    "    horizons={'day': 48, 'week': 336}\n",
    ")\n",
    "\n",
    "print(f\"   \u2713 X_cons shape: {X_cons.shape}\")\n",
    "print(f\"   \u2713 Expected: (n_sequences, 48, 17)\")\n",
    "for key, value in y_cons.items():\n",
    "    print(f\"   \u2713 y_cons['{key}'] shape: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Consumption Transformer forward pass\n",
    "print(\"\\n3. Testing Consumption Transformer forward pass (CPU)...\")\n",
    "\n",
    "model_consumption_test = ConsumptionTransformer(\n",
    "    n_features=17,\n",
    "    d_model=384,\n",
    "    n_heads=6,\n",
    "    n_layers=5,\n",
    "    horizons={'day': 48, 'week': 336}\n",
    ")\n",
    "\n",
    "# Test with small batch\n",
    "x_test = torch.FloatTensor(X_cons[:2])  # 2 samples\n",
    "with torch.no_grad():\n",
    "    output_cons = model_consumption_test(x_test)\n",
    "\n",
    "print(\"   \u2713 Forward pass successful\")\n",
    "for key, value in output_cons.items():\n",
    "    print(f\"   \u2713 {key}: {value.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model_consumption_test.parameters())\n",
    "print(f\"   \u2713 Total parameters: {total_params:,}\")\n",
    "print(f\"   \u2713 Model size: ~{total_params * 4 / (1024**2):.1f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Consumption Loss function\n",
    "print(\"\\n4. Testing Consumption Loss function...\")\n",
    "\n",
    "criterion_cons = ConsumptionLoss(horizon_weights={'day': 1.0, 'week': 0.5})\n",
    "\n",
    "targets = {\n",
    "    'consumption_day': torch.FloatTensor(y_cons['consumption_day'][:2]),\n",
    "    'consumption_week': torch.FloatTensor(y_cons['consumption_week'][:2])\n",
    "}\n",
    "\n",
    "loss, loss_dict = criterion_cons(output_cons, targets)\n",
    "print(\"   \u2713 Loss calculation successful\")\n",
    "for key, value in loss_dict.items():\n",
    "    print(f\"   \u2713 {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Gradient flow\n",
    "print(\"\\n5. Testing gradient flow (Consumption Transformer)...\")\n",
    "\n",
    "model_consumption_test.train()\n",
    "optimizer = torch.optim.AdamW(model_consumption_test.parameters(), lr=1e-4)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "output = model_consumption_test(x_test)\n",
    "loss, _ = criterion_cons(output, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"   \u2713 Gradient flow successful\")\n",
    "print(\"   \u2713 Consumption Transformer can be trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: Trading Feature Engineering\n",
    "print(\"\\n6. Testing Trading Feature Engineering...\")\n",
    "\n",
    "engineer_trading = TradingFeatureEngineer()\n",
    "\n",
    "# Get consumption predictions from test model\n",
    "with torch.no_grad():\n",
    "    test_predictions = model_consumption_test(torch.FloatTensor(X_cons[:100]))\n",
    "    consumption_preds = test_predictions['consumption_day'].numpy()\n",
    "\n",
    "# Prepare trading features\n",
    "features_trading = engineer_trading.prepare_features(\n",
    "    consumption_preds,\n",
    "    df_complete.iloc[:len(consumption_preds)]\n",
    ")\n",
    "\n",
    "print(f\"   \u2713 Trading features extracted: {features_trading.shape}\")\n",
    "print(f\"   \u2713 Expected: (n_samples, 30)\")\n",
    "assert features_trading.shape[1] == 30, f\"\u274c Expected 30 features, got {features_trading.shape[1]}\"\n",
    "\n",
    "# Check for NaN/Inf\n",
    "assert not np.isnan(features_trading).any(), \"\u274c NaN values in trading features!\"\n",
    "assert not np.isinf(features_trading).any(), \"\u274c Inf values in trading features!\"\n",
    "print(\"   \u2713 No NaN/Inf values in trading features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 7: Trading Transformer forward pass\n",
    "print(\"\\n7. Testing Trading Transformer forward pass (CPU)...\")\n",
    "\n",
    "model_trading_test = TradingTransformer(\n",
    "    n_features=30,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    n_layers=6,\n",
    "    prediction_horizon=48\n",
    ")\n",
    "\n",
    "# Create sequences for transformer input (batch, seq_len=48, features=30)\n",
    "# We need at least 48 samples to create a sequence\n",
    "if len(features_trading) >= 48:\n",
    "    # Take a sliding window of 48 timesteps\n",
    "    x_trading_test = np.array([\n",
    "        features_trading[0:48],   # First sequence\n",
    "        features_trading[1:49]    # Second sequence (shifted by 1)\n",
    "    ])\n",
    "    x_trading_test = torch.FloatTensor(x_trading_test)\n",
    "    print(f\"   Created test sequences: {x_trading_test.shape}\")\n",
    "else:\n",
    "    raise ValueError(f\"Need at least 48 samples for trading test, got {len(features_trading)}\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_trading = model_trading_test(x_trading_test)\n",
    "\n",
    "print(\"   \u2713 Forward pass successful\")\n",
    "for key, value in output_trading.items():\n",
    "    print(f\"   \u2713 {key}: {value.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params_trading = sum(p.numel() for p in model_trading_test.parameters())\n",
    "print(f\"   \u2713 Total parameters: {total_params_trading:,}\")\n",
    "print(f\"   \u2713 Model size: ~{total_params_trading * 4 / (1024**2):.1f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 8: Quick training test (2 epochs on small dataset)\n",
    "print(\"\\n8. Quick training test (2 epochs on CPU with small dataset)...\")\n",
    "print(\"   (This verifies the entire training pipeline works)\")\n",
    "\n",
    "# Create small data loaders\n",
    "train_loader_test, val_loader_test, y_keys_test = create_data_loaders(\n",
    "    X_cons[:100],\n",
    "    {k: v[:100] for k, v in y_cons.items()},\n",
    "    batch_size=8,\n",
    "    train_split=0.8\n",
    ")\n",
    "\n",
    "model_consumption_test.train()\n",
    "optimizer_test = torch.optim.AdamW(model_consumption_test.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(2):\n",
    "    train_loss, _ = train_epoch(\n",
    "        model_consumption_test,\n",
    "        train_loader_test,\n",
    "        criterion_cons,\n",
    "        optimizer_test,\n",
    "        torch.device('cpu'),\n",
    "        y_keys_test\n",
    "    )\n",
    "    val_loss, _ = validate(\n",
    "        model_consumption_test,\n",
    "        val_loader_test,\n",
    "        criterion_cons,\n",
    "        torch.device('cpu'),\n",
    "        y_keys_test\n",
    "    )\n",
    "    print(f\"   Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "print(\"   \u2713 Training pipeline works correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up test models to free memory\n",
    "print(\"\\n9. Cleaning up test models...\")\n",
    "del model_consumption_test, model_trading_test\n",
    "del x_test, x_trading_test\n",
    "del train_loader_test, val_loader_test\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"   \u2713 Memory cleaned\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 PREFLIGHT VALIDATION PASSED\")\n",
    "print(\"All systems ready for GPU training!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Train Consumption Transformer (GPU)\n",
    "\n",
    "Train the consumption prediction model on full dataset using GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SECTION 4: TRAINING CONSUMPTION TRANSFORMER\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTraining on device: {device}\")\n",
    "print(f\"Dataset size: {len(X_cons):,} sequences\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and move to GPU\n",
    "print(\"\\n1. Initializing Consumption Transformer...\")\n",
    "\n",
    "model_consumption = ConsumptionTransformer(\n",
    "    n_features=17,\n",
    "    d_model=384,\n",
    "    n_heads=6,\n",
    "    n_layers=5,\n",
    "    dim_feedforward=1536,\n",
    "    dropout=0.1,\n",
    "    horizons={'day': 48, 'week': 336}\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model_consumption.parameters())\n",
    "print(f\"   \u2713 Model created with {total_params:,} parameters\")\n",
    "print(f\"   \u2713 Model size: ~{total_params * 4 / (1024**2):.1f} MB\")\n",
    "print(f\"   \u2713 Moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "print(\"\\n2. Creating data loaders...\")\n",
    "\n",
    "train_loader, val_loader, y_keys = create_data_loaders(\n",
    "    X_cons,\n",
    "    y_cons,\n",
    "    batch_size=32,\n",
    "    train_split=0.8\n",
    ")\n",
    "\n",
    "print(f\"   \u2713 Data loaders ready\")\n",
    "print(f\"   \u2713 Training batches: {len(train_loader)}\")\n",
    "print(f\"   \u2713 Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "print(\"\\n3. Setting up training...\")\n",
    "\n",
    "criterion = ConsumptionLoss(horizon_weights={'day': 1.0, 'week': 0.5})\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model_consumption.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"   \u2713 Loss function: ConsumptionLoss (MSE weighted by horizon)\")\n",
    "print(\"   \u2713 Optimizer: AdamW (lr=1e-4, weight_decay=0.01)\")\n",
    "print(\"   \u2713 Scheduler: ReduceLROnPlateau (patience=5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"\\n4. Starting training loop...\")\n",
    "print(\"   Target: MAPE < 15% on day-ahead predictions\")\n",
    "print(\"   Early stopping: patience = 10 epochs\\n\")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_mape': [],\n",
    "    'val_mape': []\n",
    "}\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_metrics = train_epoch(\n",
    "        model_consumption,\n",
    "        train_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        device,\n",
    "        y_keys\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_metrics = validate(\n",
    "        model_consumption,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "        y_keys\n",
    "    )\n",
    "    \n",
    "    # Calculate MAPE on a small sample\n",
    "    model_consumption.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_x = torch.FloatTensor(X_cons[-100:]).to(device)\n",
    "        sample_pred = model_consumption(sample_x)['consumption_day'].cpu().numpy()\n",
    "        sample_target = y_cons['consumption_day'][-100:]\n",
    "        val_mape = calculate_mape(sample_pred, sample_target)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Track history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mape'].append(val_mape)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"Val MAPE:   {val_mape:.2f}%\")\n",
    "    print(f\"LR:         {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save checkpoint if best\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        save_checkpoint(\n",
    "            model_consumption,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            {'train_loss': train_loss, 'val_loss': val_loss, 'val_mape': val_mape},\n",
    "            '/content/drive/MyDrive/energymvp/checkpoints/consumption_transformer_best.pt'\n",
    "        )\n",
    "        print(f\"\u2713 Best model saved (val_loss: {val_loss:.4f}, MAPE: {val_mape:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping after {epoch} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Save periodic checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        save_checkpoint(\n",
    "            model_consumption,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            {'train_loss': train_loss, 'val_loss': val_loss},\n",
    "            f'/content/drive/MyDrive/energymvp/checkpoints/consumption_epoch_{epoch}.pt'\n",
    "        )\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 CONSUMPTION TRANSFORMER TRAINING COMPLETE\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Final MAPE: {val_mape:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "print(\"\\n5. Plotting training history...\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Consumption Transformer - Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# MAPE plot\n",
    "axes[1].plot(history['val_mape'], label='Val MAPE', color='orange')\n",
    "axes[1].axhline(y=15, color='r', linestyle='--', label='Target (15%)')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAPE (%)')\n",
    "axes[1].set_title('Consumption Transformer - Validation MAPE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/energymvp/checkpoints/consumption_training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"   \u2713 Training history saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Train Trading Transformer (GPU)\n",
    "\n",
    "Train the trading decision model using consumption predictions as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SECTION 5: TRAINING TRADING TRANSFORMER\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nThis model learns to make profitable trading decisions.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best consumption model\n",
    "print(\"\\n1. Loading best Consumption Transformer...\")\n",
    "\n",
    "load_checkpoint(\n",
    "    '/content/drive/MyDrive/energymvp/checkpoints/consumption_transformer_best.pt',\n",
    "    model_consumption\n",
    ")\n",
    "model_consumption.eval()\n",
    "\n",
    "print(\"   \u2713 Best consumption model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate consumption predictions for all data\n",
    "print(\"\\n2. Generating consumption predictions for training data...\")\n",
    "print(\"   (This may take a few minutes)\\n\")\n",
    "\n",
    "all_predictions = []\n",
    "batch_size = 32\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(X_cons), batch_size)):\n",
    "        batch = torch.FloatTensor(X_cons[i:i+batch_size]).to(device)\n",
    "        pred = model_consumption(batch)\n",
    "        all_predictions.append(pred['consumption_day'].cpu().numpy())\n",
    "\n",
    "consumption_predictions = np.vstack(all_predictions)\n",
    "print(f\"\\n   \u2713 Generated {len(consumption_predictions):,} predictions\")\n",
    "print(f\"   \u2713 Shape: {consumption_predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate optimal trading labels using V2 optimizer\n",
    "print(\"\\n3. Calculating optimal trading labels...\")\n",
    "print(\"   (Applying business rules to create training targets)\\n\")\n",
    "\n",
    "optimal_decisions = []\n",
    "optimal_quantities = []\n",
    "optimal_prices = []\n",
    "\n",
    "# Initialize battery at 35% SoC (room to buy)\n",
    "initial_soc = 0.35\n",
    "current_battery_charge = 40.0 * initial_soc\n",
    "\n",
    "for i in tqdm(range(len(consumption_predictions))):\n",
    "    # Battery state - track it ourselves with proper evolution\n",
    "    battery_state = {\n",
    "        'current_charge_kwh': current_battery_charge,\n",
    "        'capacity_kwh': 40.0,\n",
    "        'min_soc': 0.20,\n",
    "        'max_soc': 1.0,  # Changed from 0.90\n",
    "        'max_charge_rate_kw': 10.0,\n",
    "        'max_discharge_rate_kw': 8.0,\n",
    "        'efficiency': 0.95\n",
    "    }\n",
    "    \n",
    "    # Get pricing data for next 48 intervals\n",
    "    price_data = pricing_df['price_per_kwh'].values[i:i+48] if i+48 <= len(pricing_df) else pricing_df['price_per_kwh'].values[-48:]\n",
    "    \n",
    "    # Calculate optimal trading decision using V2 optimizer\n",
    "    labels = calculate_optimal_trading_decisions(\n",
    "        predicted_consumption=consumption_predictions[i],\n",
    "        actual_prices=price_data,\n",
    "        battery_state=battery_state,\n",
    "        household_price_kwh=0.27,\n",
    "        buy_threshold_mwh=20.0,\n",
    "        sell_threshold_mwh=40.0,\n",
    "        min_soc_for_sell=0.25,\n",
    "        target_soc_on_buy=0.90\n",
    "    )\n",
    "    \n",
    "    optimal_decisions.append(labels['optimal_decisions'])\n",
    "    optimal_quantities.append(labels['optimal_quantities'])\n",
    "    optimal_prices.append(price_data[0])\n",
    "    \n",
    "    # Update battery charge based on first decision\n",
    "    decision = labels['optimal_decisions'][0]\n",
    "    quantity = labels['optimal_quantities'][0]\n",
    "    \n",
    "    if decision == 0:  # Buy\n",
    "        current_battery_charge += quantity * battery_state['efficiency']\n",
    "    elif decision == 2:  # Sell\n",
    "        current_battery_charge -= quantity\n",
    "    \n",
    "    # CRITICAL: Subtract consumption\n",
    "    current_battery_charge -= consumption_predictions[i][0]\n",
    "    \n",
    "    # Clip to bounds\n",
    "    current_soc = current_battery_charge / 40.0\n",
    "    current_soc = np.clip(current_soc, 0.20, 1.0)\n",
    "    current_battery_charge = 40.0 * current_soc\n",
    "\n",
    "optimal_decisions = np.array(optimal_decisions)\n",
    "optimal_quantities = np.array(optimal_quantities)\n",
    "optimal_prices = np.array(optimal_prices)\n",
    "\n",
    "print(f\"\\n   \u2713 Calculated {len(optimal_decisions):,} optimal trading labels\")\n",
    "print(f\"   \u2713 Decision distribution:\")\n",
    "unique, counts = np.unique(optimal_decisions[:, 0], return_counts=True)\n",
    "for decision, count in zip(unique, counts):\n",
    "    decision_name = ['Buy', 'Hold', 'Sell'][int(decision)]\n",
    "    print(f\"      - {decision_name}: {count} ({count/len(optimal_decisions)*100:.1f}%)\")\n",
    "\n",
    "print(f\"   \u2713 Final battery SoC: {current_soc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare trading features\n",
    "print(\"\\n4. Preparing trading features...\")\n",
    "\n",
    "features_trading = engineer_trading.prepare_features(\n",
    "    consumption_predictions,\n",
    "    df_complete.iloc[:len(consumption_predictions)]\n",
    ")\n",
    "\n",
    "print(f\"   \u2713 Trading features prepared: {features_trading.shape}\")\n",
    "print(f\"   \u2713 Expected: (n_samples, 30)\")\n",
    "\n",
    "# Reshape for transformer (batch, seq_len=48, features=30)\n",
    "# For now, we'll use a sliding window approach\n",
    "# Each sample uses the last 48 feature vectors\n",
    "n_samples_trading = len(features_trading) - 48\n",
    "X_trading = np.zeros((n_samples_trading, 48, 30))\n",
    "\n",
    "for i in range(n_samples_trading):\n",
    "    X_trading[i] = features_trading[i:i+48]\n",
    "\n",
    "# Align targets\n",
    "y_trading = {\n",
    "    'price': optimal_prices[:n_samples_trading],\n",
    "    'decisions': optimal_decisions[:n_samples_trading, 0],  # Next interval decision\n",
    "    'quantities': optimal_quantities[:n_samples_trading, 0],  # Next interval quantity\n",
    "    'consumption': consumption_predictions[:n_samples_trading, 0]  # For profit calculation\n",
    "}\n",
    "\n",
    "print(f\"   \u2713 Trading sequences created: {X_trading.shape}\")\n",
    "print(f\"   \u2713 Target shapes:\")\n",
    "for key, value in y_trading.items():\n",
    "    print(f\"      - {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trading Transformer\n",
    "print(\"\\n5. Initializing Trading Transformer...\")\n",
    "\n",
    "model_trading = TradingTransformer(\n",
    "    n_features=30,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    n_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    prediction_horizon=48\n",
    ").to(device)\n",
    "\n",
    "total_params_trading = sum(p.numel() for p in model_trading.parameters())\n",
    "print(f\"   \u2713 Model created with {total_params_trading:,} parameters\")\n",
    "print(f\"   \u2713 Model size: ~{total_params_trading * 4 / (1024**2):.1f} MB\")\n",
    "print(f\"   \u2713 Moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for trading\n",
    "print(\"\\n6. Creating data loaders for trading...\")\n",
    "\n",
    "train_loader_trading, val_loader_trading, y_keys_trading = create_data_loaders(\n",
    "    X_trading,\n",
    "    y_trading,\n",
    "    batch_size=32,\n",
    "    train_split=0.8\n",
    ")\n",
    "\n",
    "print(f\"   \u2713 Data loaders ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training for Trading Transformer\n",
    "print(\"\\n7. Setting up trading training...\")\n",
    "\n",
    "criterion_trading = TradingLoss(\n",
    "    price_weight=0.20,\n",
    "    decision_weight=0.20,\n",
    "    profit_weight=0.60,\n",
    "    household_price_kwh=0.27\n",
    ")\n",
    "\n",
    "optimizer_trading = torch.optim.AdamW(\n",
    "    model_trading.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "scheduler_trading = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_trading,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"   \u2713 Loss: TradingLoss (20% price + 20% decision + 60% profit)\")\n",
    "print(\"   \u2713 Optimizer: AdamW (lr=1e-4)\")\n",
    "print(\"   \u2713 Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trading Transformer training loop\n",
    "print(\"\\n8. Starting trading training loop...\")\n",
    "print(\"   Target: Price MAE < $0.05/kWh, Cost savings 20-40%\")\n",
    "print(\"   Early stopping: patience = 10 epochs\\n\")\n",
    "\n",
    "num_epochs_trading = 100\n",
    "best_val_loss_trading = float('inf')\n",
    "patience_trading = 10\n",
    "patience_counter_trading = 0\n",
    "\n",
    "history_trading = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'price_mae': [],\n",
    "    'avg_profit': []\n",
    "}\n",
    "\n",
    "for epoch in range(1, num_epochs_trading + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{num_epochs_trading}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_metrics = train_epoch(\n",
    "        model_trading,\n",
    "        train_loader_trading,\n",
    "        criterion_trading,\n",
    "        optimizer_trading,\n",
    "        device,\n",
    "        y_keys_trading\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_metrics = validate(\n",
    "        model_trading,\n",
    "        val_loader_trading,\n",
    "        criterion_trading,\n",
    "        device,\n",
    "        y_keys_trading\n",
    "    )\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler_trading.step(val_loss)\n",
    "    \n",
    "    # Track history\n",
    "    history_trading['train_loss'].append(train_loss)\n",
    "    history_trading['val_loss'].append(val_loss)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"LR:         {optimizer_trading.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save checkpoint if best\n",
    "    if val_loss < best_val_loss_trading:\n",
    "        best_val_loss_trading = val_loss\n",
    "        patience_counter_trading = 0\n",
    "        \n",
    "        save_checkpoint(\n",
    "            model_trading,\n",
    "            optimizer_trading,\n",
    "            epoch,\n",
    "            {'train_loss': train_loss, 'val_loss': val_loss},\n",
    "            '/content/drive/MyDrive/energymvp/checkpoints/trading_transformer_best.pt'\n",
    "        )\n",
    "        print(f\"\u2713 Best trading model saved (val_loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter_trading += 1\n",
    "        print(f\"Patience: {patience_counter_trading}/{patience_trading}\")\n",
    "        \n",
    "        if patience_counter_trading >= patience_trading:\n",
    "            print(f\"\\nEarly stopping after {epoch} epochs\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 TRADING TRANSFORMER TRAINING COMPLETE\")\n",
    "print(f\"Best validation loss: {best_val_loss_trading:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trading training history\n",
    "print(\"\\n9. Plotting trading training history...\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.plot(history_trading['train_loss'], label='Train Loss')\n",
    "ax.plot(history_trading['val_loss'], label='Val Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Trading Transformer - Training Loss')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/energymvp/checkpoints/trading_training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"   \u2713 Training history saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: End-to-End Testing\n",
    "\n",
    "Validate the complete pipeline with comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SECTION 6: END-TO-END TESTING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRunning complete inference pipeline on test data...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best models\n",
    "print(\"\\n1. Loading best models...\")\n",
    "\n",
    "load_checkpoint(\n",
    "    '/content/drive/MyDrive/energymvp/checkpoints/consumption_transformer_best.pt',\n",
    "    model_consumption\n",
    ")\n",
    "load_checkpoint(\n",
    "    '/content/drive/MyDrive/energymvp/checkpoints/trading_transformer_best.pt',\n",
    "    model_trading\n",
    ")\n",
    "\n",
    "model_consumption.eval()\n",
    "model_trading.eval()\n",
    "\n",
    "print(\"   \u2713 Both models loaded and set to eval mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test set\n",
    "print(\"\\n2. Running inference on test samples...\")\n",
    "\n",
    "test_size = 100\n",
    "test_X_cons = torch.FloatTensor(X_cons[-test_size:]).to(device)\n",
    "test_X_trading = torch.FloatTensor(X_trading[-test_size:]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Consumption predictions\n",
    "    consumption_pred = model_consumption(test_X_cons)\n",
    "    \n",
    "    # Trading predictions\n",
    "    trading_pred = model_trading(test_X_trading)\n",
    "\n",
    "print(\"   \u2713 Inference complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics\n",
    "print(\"\\n3. Calculating metrics...\")\n",
    "\n",
    "# Consumption MAPE\n",
    "cons_pred_day = consumption_pred['consumption_day'].cpu().numpy()\n",
    "cons_target_day = y_cons['consumption_day'][-test_size:]\n",
    "consumption_mape = calculate_mape(cons_pred_day, cons_target_day)\n",
    "\n",
    "# Price MAE\n",
    "price_pred = trading_pred['predicted_price'][:, 0].cpu().numpy()\n",
    "price_target = y_trading['price'][-test_size:]\n",
    "price_mae = np.mean(np.abs(price_pred - price_target))\n",
    "\n",
    "# Trading decision accuracy\n",
    "decision_pred = torch.argmax(trading_pred['trading_decisions'][:, 0, :], dim=1).cpu().numpy()\n",
    "decision_target = y_trading['decisions'][-test_size:]\n",
    "decision_accuracy = (decision_pred == decision_target).mean() * 100\n",
    "\n",
    "print(f\"\\n   \ud83d\udcca PERFORMANCE METRICS:\")\n",
    "print(f\"   {'='*50}\")\n",
    "print(f\"   Consumption MAPE:      {consumption_mape:.2f}% {'\u2713' if consumption_mape < 15 else '\u274c'} (target < 15%)\")\n",
    "print(f\"   Price MAE:             ${price_mae:.4f} {'\u2713' if price_mae < 0.05 else '\u274c'} (target < $0.05)\")\n",
    "print(f\"   Trading Accuracy:      {decision_accuracy:.2f}%\")\n",
    "print(f\"   {'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cost savings vs baseline\n",
    "print(\"\\n4. Calculating cost savings vs baseline...\")\n",
    "\n",
    "# Baseline: buy all energy from grid at market price\n",
    "baseline_cost = (cons_target_day * price_target[:, np.newaxis]).sum()\n",
    "\n",
    "# Optimized: use trading decisions\n",
    "# This is a simplified calculation - full simulation would be more complex\n",
    "quantities_pred = trading_pred['trade_quantities'][:, 0].cpu().numpy()\n",
    "buy_mask = (decision_pred == 0)\n",
    "sell_mask = (decision_pred == 2)\n",
    "\n",
    "trading_cost = baseline_cost  # Start with baseline\n",
    "trading_cost -= (quantities_pred[sell_mask] * price_target[sell_mask]).sum()  # Revenue from selling\n",
    "trading_cost += (quantities_pred[buy_mask] * price_target[buy_mask]).sum()  # Cost of buying\n",
    "\n",
    "savings = baseline_cost - trading_cost\n",
    "savings_percent = (savings / baseline_cost) * 100\n",
    "\n",
    "print(f\"\\n   \ud83d\udcb0 COST ANALYSIS:\")\n",
    "print(f\"   {'='*50}\")\n",
    "print(f\"   Baseline Cost:         ${baseline_cost:.2f}\")\n",
    "print(f\"   Optimized Cost:        ${trading_cost:.2f}\")\n",
    "print(f\"   Savings:               ${savings:.2f}\")\n",
    "print(f\"   Savings Percentage:    {savings_percent:.2f}% {'\u2713' if 20 <= savings_percent <= 40 else '\u26a0\ufe0f'} (target 20-40%)\")\n",
    "print(f\"   {'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trading decision confusion matrix\n",
    "print(\"\\n5. Trading decision distribution...\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(decision_target, decision_pred)\n",
    "labels = ['Buy', 'Hold', 'Sell']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Trading Decision Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/energymvp/checkpoints/confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"   \u2713 Confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions\n",
    "print(\"\\n6. Visualizing sample predictions...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Consumption prediction\n",
    "sample_idx = 0\n",
    "axes[0, 0].plot(cons_target_day[sample_idx], label='Actual', marker='o', markersize=3)\n",
    "axes[0, 0].plot(cons_pred_day[sample_idx], label='Predicted', marker='x', markersize=3)\n",
    "axes[0, 0].set_xlabel('Time Interval')\n",
    "axes[0, 0].set_ylabel('Consumption (kWh)')\n",
    "axes[0, 0].set_title('Consumption Prediction (Next 24 Hours)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Price prediction\n",
    "axes[0, 1].scatter(price_target, price_pred, alpha=0.5)\n",
    "axes[0, 1].plot([price_target.min(), price_target.max()], \n",
    "                [price_target.min(), price_target.max()], \n",
    "                'r--', label='Perfect Prediction')\n",
    "axes[0, 1].set_xlabel('Actual Price ($/kWh)')\n",
    "axes[0, 1].set_ylabel('Predicted Price ($/kWh)')\n",
    "axes[0, 1].set_title('Price Prediction Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Trading decisions over time\n",
    "axes[1, 0].plot(decision_pred[:50], marker='o', label='Predicted')\n",
    "axes[1, 0].plot(decision_target[:50], marker='x', label='Optimal', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Time Interval')\n",
    "axes[1, 0].set_ylabel('Decision (0=Buy, 1=Hold, 2=Sell)')\n",
    "axes[1, 0].set_title('Trading Decisions (First 50 Intervals)')\n",
    "axes[1, 0].set_yticks([0, 1, 2])\n",
    "axes[1, 0].set_yticklabels(['Buy', 'Hold', 'Sell'])\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Training history comparison\n",
    "axes[1, 1].plot(history['val_loss'], label='Consumption Val Loss')\n",
    "axes[1, 1].plot(history_trading['val_loss'], label='Trading Val Loss')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].set_title('Training Convergence')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/energymvp/checkpoints/end_to_end_results.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"   \u2713 Visualizations saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 END-TO-END TESTING COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Model Export & Summary\n",
    "\n",
    "Save model configurations, training logs, and generate final summary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SECTION 7: MODEL EXPORT & SUMMARY\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model configurations\n",
    "print(\"\\n1. Saving model configurations...\")\n",
    "\n",
    "import json\n",
    "\n",
    "config_consumption = {\n",
    "    'architecture': 'ConsumptionTransformer',\n",
    "    'n_features': 17,\n",
    "    'd_model': 384,\n",
    "    'n_heads': 6,\n",
    "    'n_layers': 5,\n",
    "    'dim_feedforward': 1536,\n",
    "    'horizons': {'day': 48, 'week': 336},\n",
    "    'total_parameters': sum(p.numel() for p in model_consumption.parameters())\n",
    "}\n",
    "\n",
    "config_trading = {\n",
    "    'architecture': 'TradingTransformer',\n",
    "    'n_features': 30,\n",
    "    'd_model': 512,\n",
    "    'n_heads': 8,\n",
    "    'n_layers': 6,\n",
    "    'dim_feedforward': 2048,\n",
    "    'prediction_horizon': 48,\n",
    "    'total_parameters': sum(p.numel() for p in model_trading.parameters())\n",
    "}\n",
    "\n",
    "with open('/content/drive/MyDrive/energymvp/checkpoints/consumption_config.json', 'w') as f:\n",
    "    json.dump(config_consumption, f, indent=2)\n",
    "\n",
    "with open('/content/drive/MyDrive/energymvp/checkpoints/trading_config.json', 'w') as f:\n",
    "    json.dump(config_trading, f, indent=2)\n",
    "\n",
    "print(\"   \u2713 Model configurations saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training logs\n",
    "print(\"\\n2. Saving training logs...\")\n",
    "\n",
    "training_logs = {\n",
    "    'consumption': {\n",
    "        'train_loss_history': [float(x) for x in history['train_loss']],\n",
    "        'val_loss_history': [float(x) for x in history['val_loss']],\n",
    "        'val_mape_history': [float(x) for x in history['val_mape']],\n",
    "        'best_val_loss': float(best_val_loss),\n",
    "        'final_mape': float(consumption_mape),\n",
    "        'epochs_trained': len(history['train_loss'])\n",
    "    },\n",
    "    'trading': {\n",
    "        'train_loss_history': [float(x) for x in history_trading['train_loss']],\n",
    "        'val_loss_history': [float(x) for x in history_trading['val_loss']],\n",
    "        'best_val_loss': float(best_val_loss_trading),\n",
    "        'epochs_trained': len(history_trading['train_loss'])\n",
    "    },\n",
    "    'evaluation': {\n",
    "        'consumption_mape': float(consumption_mape),\n",
    "        'price_mae': float(price_mae),\n",
    "        'trading_accuracy': float(decision_accuracy),\n",
    "        'baseline_cost': float(baseline_cost),\n",
    "        'optimized_cost': float(trading_cost),\n",
    "        'savings': float(savings),\n",
    "        'savings_percent': float(savings_percent)\n",
    "    },\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'device': str(device)\n",
    "}\n",
    "\n",
    "with open('/content/drive/MyDrive/energymvp/checkpoints/training_logs.json', 'w') as f:\n",
    "    json.dump(training_logs, f, indent=2)\n",
    "\n",
    "print(\"   \u2713 Training logs saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary report\n",
    "print(\"\\n3. Generating summary report...\")\n",
    "\n",
    "summary = f\"\"\"\n",
    "{'='*70}\n",
    "DUAL-TRANSFORMER ENERGY TRADING SYSTEM\n",
    "Training Summary Report\n",
    "{'='*70}\n",
    "\n",
    "Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Device: {device}\n",
    "\n",
    "{'='*70}\n",
    "CONSUMPTION TRANSFORMER\n",
    "{'='*70}\n",
    "Architecture:        {config_consumption['d_model']}d, {config_consumption['n_heads']} heads, {config_consumption['n_layers']} layers\n",
    "Parameters:          {config_consumption['total_parameters']:,}\n",
    "Input Features:      {config_consumption['n_features']}\n",
    "Output Horizons:     Day (48), Week (336)\n",
    "\n",
    "Training:\n",
    "  Epochs:            {len(history['train_loss'])}\n",
    "  Best Val Loss:     {best_val_loss:.4f}\n",
    "  Final MAPE:        {consumption_mape:.2f}%\n",
    "  Target:            < 15% {'\u2713 PASSED' if consumption_mape < 15 else '\u274c NOT MET'}\n",
    "\n",
    "{'='*70}\n",
    "TRADING TRANSFORMER\n",
    "{'='*70}\n",
    "Architecture:        {config_trading['d_model']}d, {config_trading['n_heads']} heads, {config_trading['n_layers']} layers\n",
    "Parameters:          {config_trading['total_parameters']:,}\n",
    "Input Features:      {config_trading['n_features']}\n",
    "Output:              Price + Decision + Quantity\n",
    "\n",
    "Training:\n",
    "  Epochs:            {len(history_trading['train_loss'])}\n",
    "  Best Val Loss:     {best_val_loss_trading:.4f}\n",
    "\n",
    "{'='*70}\n",
    "EVALUATION METRICS\n",
    "{'='*70}\n",
    "Consumption MAPE:    {consumption_mape:.2f}% {'\u2713' if consumption_mape < 15 else '\u274c'} (target < 15%)\n",
    "Price MAE:           ${price_mae:.4f} {'\u2713' if price_mae < 0.05 else '\u274c'} (target < $0.05)\n",
    "Trading Accuracy:    {decision_accuracy:.2f}%\n",
    "\n",
    "Cost Analysis:\n",
    "  Baseline Cost:     ${baseline_cost:.2f}\n",
    "  Optimized Cost:    ${trading_cost:.2f}\n",
    "  Savings:           ${savings:.2f}\n",
    "  Savings %:         {savings_percent:.2f}% {'\u2713' if 20 <= savings_percent <= 40 else '\u26a0\ufe0f'} (target 20-40%)\n",
    "\n",
    "{'='*70}\n",
    "SUCCESS CRITERIA\n",
    "{'='*70}\n",
    "\u2713 Consumption MAPE < 15%:     {'\u2713 PASSED' if consumption_mape < 15 else '\u274c NOT MET'}\n",
    "\u2713 Price MAE < $0.05/kWh:      {'\u2713 PASSED' if price_mae < 0.05 else '\u274c NOT MET'}\n",
    "\u2713 Cost savings 20-40%:        {'\u2713 PASSED' if 20 <= savings_percent <= 40 else '\u26a0\ufe0f NOT MET'}\n",
    "\u2713 Models converged:           \u2713 PASSED\n",
    "\u2713 Checkpoints saved:          \u2713 PASSED\n",
    "\n",
    "{'='*70}\n",
    "FILES SAVED\n",
    "{'='*70}\n",
    "Checkpoints:\n",
    "  - consumption_transformer_best.pt\n",
    "  - trading_transformer_best.pt\n",
    "\n",
    "Configurations:\n",
    "  - consumption_config.json\n",
    "  - trading_config.json\n",
    "\n",
    "Logs:\n",
    "  - training_logs.json\n",
    "\n",
    "Visualizations:\n",
    "  - consumption_training_history.png\n",
    "  - trading_training_history.png\n",
    "  - confusion_matrix.png\n",
    "  - end_to_end_results.png\n",
    "\n",
    "{'='*70}\n",
    "NEXT STEPS\n",
    "{'='*70}\n",
    "1. Download checkpoints from Google Drive\n",
    "2. Test models on local machine\n",
    "3. Deploy to production environment\n",
    "4. Monitor performance metrics\n",
    "5. Retrain periodically with new data\n",
    "\n",
    "{'='*70}\n",
    "TRAINING COMPLETE\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary to file\n",
    "with open('/content/drive/MyDrive/energymvp/checkpoints/TRAINING_SUMMARY.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\n   \u2713 Summary report saved to TRAINING_SUMMARY.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final memory cleanup\n",
    "print(\"\\n4. Final cleanup...\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU memory used: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"   GPU memory after cleanup: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 ALL TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAll models, logs, and visualizations saved to:\")\n",
    "print(\"/content/drive/MyDrive/energymvp/checkpoints/\")\n",
    "print(\"\\nReady for deployment!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}